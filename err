wandb: Appending key for api.wandb.ai to your netrc file: /home/yifeizuo/.netrc
wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin
Creating parquet from Arrow format:   0%|          | 0/456 [00:00<?, ?ba/s]Creating parquet from Arrow format:  11%|█         | 51/456 [00:00<00:00, 509.33ba/s]Creating parquet from Arrow format:  23%|██▎       | 103/456 [00:00<00:00, 512.22ba/s]Creating parquet from Arrow format:  34%|███▍      | 156/456 [00:00<00:00, 517.87ba/s]Creating parquet from Arrow format:  47%|████▋     | 213/456 [00:00<00:00, 538.27ba/s]Creating parquet from Arrow format:  61%|██████    | 277/456 [00:00<00:00, 574.74ba/s]Creating parquet from Arrow format:  75%|███████▍  | 341/456 [00:00<00:00, 595.49ba/s]Creating parquet from Arrow format:  89%|████████▉ | 406/456 [00:00<00:00, 610.41ba/s]Creating parquet from Arrow format: 100%|██████████| 456/456 [00:00<00:00, 583.25ba/s]
Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 659.69ba/s]
Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 266.78ba/s]
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 1024 examples [00:00, 180741.80 examples/s]
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 500 examples [00:00, 193714.39 examples/s]
Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 582.95ba/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 42000 examples [00:00, 412664.66 examples/s]Generating train split: 92000 examples [00:00, 461767.50 examples/s]Generating train split: 145000 examples [00:00, 488836.58 examples/s]Generating train split: 197000 examples [00:00, 493895.16 examples/s]Generating train split: 260000 examples [00:00, 459050.74 examples/s]Generating train split: 309000 examples [00:00, 464563.03 examples/s]Generating train split: 359000 examples [00:00, 473592.46 examples/s]Generating train split: 409000 examples [00:00, 478413.37 examples/s]Generating train split: 455261 examples [00:00, 469575.06 examples/s]
Creating parquet from Arrow format:   0%|          | 0/456 [00:00<?, ?ba/s]Creating parquet from Arrow format:  11%|█         | 48/456 [00:00<00:00, 475.65ba/s]Creating parquet from Arrow format:  22%|██▏       | 101/456 [00:00<00:00, 504.14ba/s]Creating parquet from Arrow format:  34%|███▍      | 154/456 [00:00<00:00, 513.13ba/s]Creating parquet from Arrow format:  45%|████▌     | 207/456 [00:00<00:00, 516.82ba/s]Creating parquet from Arrow format:  57%|█████▋    | 260/456 [00:00<00:00, 519.60ba/s]Creating parquet from Arrow format:  69%|██████▊   | 313/456 [00:00<00:00, 521.44ba/s]Creating parquet from Arrow format:  80%|████████  | 366/456 [00:00<00:00, 519.80ba/s]Creating parquet from Arrow format:  92%|█████████▏| 419/456 [00:00<00:00, 520.73ba/s]Creating parquet from Arrow format: 100%|██████████| 456/456 [00:00<00:00, 518.27ba/s]
2025-03-05 16:32:25,163	INFO worker.py:1832 -- Started a local Ray instance. View the dashboard at [1m[32mhttp://127.0.0.1:8265 [39m[22m
[36m(WorkerDict pid=160935)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=160764)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=160941)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(WorkerDict pid=160935)[0m /home/yifeizuo/yifeizuo/verl/briter/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=160935)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[36m(WorkerDict pid=160935)[0m /home/yifeizuo/yifeizuo/verl/briter/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=160935)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36m(WorkerDict pid=160938)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=160938)[0m /home/yifeizuo/yifeizuo/verl/briter/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=160938)[0m   warnings.warn(
[36m(WorkerDict pid=160941)[0m /home/yifeizuo/yifeizuo/verl/briter/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.[32m [repeated 14x across cluster][0m
[36m(WorkerDict pid=160941)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=160941)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")[32m [repeated 7x across cluster][0m
[36m(main_task pid=158689)[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
[36m(main_task pid=158689)[0m wandb: Currently logged in as: yifeizuo2029 (yifeizuo2029-northwestern-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(main_task pid=158689)[0m wandb: Tracking run with wandb version 0.19.7
[36m(main_task pid=158689)[0m wandb: Run data is saved locally in /home/yifeizuo/yifeizuo/verl/wandb/run-20250305_163552-ndt0u4kj
[36m(main_task pid=158689)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(main_task pid=158689)[0m wandb: Syncing run ppo
[36m(main_task pid=158689)[0m wandb: ⭐️ View project at https://wandb.ai/yifeizuo2029-northwestern-university/Qwen2.5-Math-1.5B
[36m(main_task pid=158689)[0m wandb: 🚀 View run at https://wandb.ai/yifeizuo2029-northwestern-university/Qwen2.5-Math-1.5B/runs/ndt0u4kj
[36m(WorkerDict pid=160764)[0m /home/yifeizuo/yifeizuo/verl/briter/lib/python3.11/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
[36m(WorkerDict pid=160764)[0m   with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
[36m(WorkerDict pid=160941)[0m /home/yifeizuo/yifeizuo/verl/briter/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=160941)[0m   warnings.warn([32m [repeated 7x across cluster][0m
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/yifeizuo/yifeizuo/verl/verl/trainer/main_ppo.py", line 134, in <module>
    main()
  File "/home/yifeizuo/yifeizuo/verl/briter/lib/python3.11/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/yifeizuo/yifeizuo/verl/briter/lib/python3.11/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/yifeizuo/yifeizuo/verl/briter/lib/python3.11/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/yifeizuo/yifeizuo/verl/briter/lib/python3.11/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
           ^^^^^^
  File "/home/yifeizuo/yifeizuo/verl/briter/lib/python3.11/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
            ^^^^^^^^^^
  File "/home/yifeizuo/yifeizuo/verl/briter/lib/python3.11/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
          ^^^^^^^^
  File "/home/yifeizuo/yifeizuo/verl/briter/lib/python3.11/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
                       ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yifeizuo/yifeizuo/verl/verl/trainer/main_ppo.py", line 25, in main
    run_ppo(config)
  File "/home/yifeizuo/yifeizuo/verl/verl/trainer/main_ppo.py", line 38, in run_ppo
    ray.get(main_task.remote(config, compute_score))
  File "/home/yifeizuo/yifeizuo/verl/briter/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/yifeizuo/yifeizuo/verl/briter/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/yifeizuo/yifeizuo/verl/briter/lib/python3.11/site-packages/ray/_private/worker.py", line 2771, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yifeizuo/yifeizuo/verl/briter/lib/python3.11/site-packages/ray/_private/worker.py", line 893, in get_objects
    ] = self.core_worker.get_objects(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "python/ray/_raylet.pyx", line 3189, in ray._raylet.CoreWorker.get_objects
  File "python/ray/includes/common.pxi", line 83, in ray._raylet.check_status
KeyboardInterrupt
Exception ignored in atexit callback: <function shutdown at 0x7f09f5a465c0>
Traceback (most recent call last):
  File "/home/yifeizuo/yifeizuo/verl/briter/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/yifeizuo/yifeizuo/verl/briter/lib/python3.11/site-packages/ray/_private/worker.py", line 1910, in shutdown
    time.sleep(0.5)
KeyboardInterrupt: 
